% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lmCombine.R
\name{lmCombine}
\alias{lmCombine}
\title{Combine regressions based on information criteria}
\usage{
lmCombine(data, ic = c("AICc", "AIC", "BIC", "BICc"), bruteforce = FALSE,
  silent = TRUE, formula = NULL, subset = NULL,
  distribution = c("dnorm", "dlaplace", "ds", "dgnorm", "dlogis", "dt",
  "dalaplace", "dlnorm", "dllaplace", "dls", "dlgnorm", "dbcnorm", "dinvgauss",
  "dgamma", "dexp", "dfnorm", "drectnorm", "dpois", "dnbinom", "dbeta",
  "dlogitnorm", "plogis", "pnorm"), parallel = FALSE, ...)
}
\arguments{
\item{data}{Data frame containing dependent variable in the first column and
the others in the rest.}

\item{ic}{Information criterion to use.}

\item{bruteforce}{If \code{TRUE}, then all the possible models are generated
and combined. Otherwise the best model is found and then models around that
one are produced and then combined.}

\item{silent}{If \code{FALSE}, then nothing is silent, everything is printed
out. \code{TRUE} means that nothing is produced.}

\item{formula}{If provided, then the selection will be done from the listed
variables in the formula after all the necessary transformations.}

\item{subset}{an optional vector specifying a subset of observations to be
used in the fitting process.}

\item{distribution}{Distribution to pass to \code{alm()}. See \link[greybox]{alm}
for details.}

\item{parallel}{If \code{TRUE}, then the model fitting is done in parallel.
WARNING! Packages \code{foreach} and either \code{doMC} (Linux and Mac only)
or \code{doParallel} are needed in order to run the function in parallel.}

\item{...}{Other parameters passed to \code{alm()}.}
}
\value{
Function returns \code{model} - the final model of the class
"greyboxC". The list of variables:
\itemize{
\item coefficients - combined parameters of the model,
\item vcov - combined covariance matrix of the model,
\item fitted - the fitted values,
\item residuals - residual of the model,
\item distribution - distribution used in the estimation,
\item logLik - combined log-likelihood of the model,
\item IC - the values of the combined information criterion,
\item ICType - the type of information criterion used,
\item df.residual - number of degrees of freedom of the residuals of
the combined model,
\item df - number of degrees of freedom of the combined model,
\item importance - importance of the parameters,
\item combination - the table, indicating which variables were used in every
model construction and what were the weights for each model,
\item timeElapsed - the time elapsed for the estimation of the model.
}
}
\description{
Function combines parameters of linear regressions of the first variable
on all the other provided data.
}
\details{
The algorithm uses alm() to fit different models and then combines the models
based on the selected IC. The parameters are combined so that if they are not
present in some of models, it is assumed that they are equal to zero. Thus,
there is a shrinkage effect in the combination.

Some details and examples of application are also given in the vignette
"Greybox": \code{vignette("greybox","greybox")}
}
\examples{

### Simple example
xreg <- cbind(rnorm(100,10,3),rnorm(100,50,5))
xreg <- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rnorm(100,0,3),xreg,rnorm(100,300,10))
colnames(xreg) <- c("y","x1","x2","Noise")
inSample <- xreg[1:80,]
outSample <- xreg[-c(1:80),]
# Combine all the possible models
ourModel <- lmCombine(inSample,bruteforce=TRUE)
predict(ourModel,outSample)
plot(predict(ourModel,outSample))

### Fat regression example
xreg <- matrix(rnorm(5000,10,3),50,100)
xreg <- cbind(100+0.5*xreg[,1]-0.75*xreg[,2]+rnorm(50,0,3),xreg,rnorm(50,300,10))
colnames(xreg) <- c("y",paste0("x",c(1:100)),"Noise")
inSample <- xreg[1:40,]
outSample <- xreg[-c(1:40),]
# Combine only the models close to the optimal
ourModel <- lmCombine(inSample, ic="BICc",bruteforce=FALSE)
summary(ourModel)
plot(predict(ourModel, outSample))

# Combine in parallel - should increase speed in case of big data
\dontrun{ourModel <- lmCombine(inSample, ic="BICc", bruteforce=TRUE, parallel=TRUE)
summary(ourModel)
plot(predict(ourModel, outSample))}

}
\references{
\itemize{
\item Burnham Kenneth P. and Anderson David R. (2002). Model Selection
and Multimodel Inference. A Practical Information-Theoretic Approach.
Springer-Verlag New York. DOI: [10.1007/b97636](http://dx.doi.org/10.1007/b97636).
\item McQuarrie, A. D. (1999). A small-sample correction for the Schwarz SIC model
selection criterion. Statistics {&} Probability Letters, 44(1), 79â€“86.
[10.1016/S0167-7152(98)00294-6](https://doi.org/10.1016/S0167-7152(98)00294-6).
}
}
\seealso{
\code{\link[stats]{step}, \link[greybox]{xregExpander},
\link[greybox]{stepwise}}
}
\author{
Ivan Svetunkov, \email{ivan@svetunkov.ru}
}
\keyword{models}
\keyword{nonlinear}
\keyword{regression}
\keyword{ts}
